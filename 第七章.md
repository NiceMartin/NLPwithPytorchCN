
## 第七章 用于自然语言处理的中间序列建模

本章的目标是序列预测。序列预测任务要求我们对序列中的每一项进行标记，这类任务在自然语言处理中很常见
语言建模(见igure 7­1),我们预测下一个单词给出一系列单词每一步;那一部分­­演讲的标签,我们预测每个词的语法词性;命名实体识别,我们预测每个单词是否命名实体的一部分,等
人员、位置、产品或组织的名称等等。在NLP文献中，序列预测任务有时也称为序列标记。
路径



自由党再次



年代




虽然在理论上我们可以使用在第6章中引入的Elman递归神经网络来进行
在序列预测任务中，它们不能很好地捕获长期依赖关系，并且执行得很差
练习。在本章中，我们将花一些时间来理解为什么会这样，并学习一种称为门控网络的新型RNN架构。
我们还介绍了自然语言生成作为序列预测应用的任务，并探讨了输出序列在某种程度上受到约束的条件生成。

C
 


图7-1序列预测任务的两个例子:(a)语言建模，其中的任务是预测序列中的下一个单词;(b)命名实体识别，其目的是预测序列中实体字符串的边界
文本及其类型。
香草RNNs(或Elman RNNs)的问题
尽管在hapter 6中讨论的vanilla/Elman RNN非常适合于序列建模，但是它有两个问题使得它不适合很多任务:无法为长期预测保留信息，以及梯度稳定性
使用前一个时间步长的隐藏状态向量和当前时间步长的输入向量计算每个时间步长的隐藏状态向量。正是这种核心计算使得RNN如此强大，但它也产生了剧烈的数值问题。
Elman RNNs的第一个问题是很难保持长期的信息。例如，对于hater6中的RNN，在每个时间步长时，我们只更新隐藏的状态向量，而不考虑
是否有意义。因此，在完全由输入决定的隐藏状态下，RNN无法控制哪些值被保留，哪些值被丢弃
没有意义。我们希望RNN通过某种方式来决定更新是可选的，还是发生了更新，通过状态向量的多少和哪些部分，等等。
Elman RNNs的第二个问题是，它们会导致梯度失控地旋转到零或无穷大，不稳定的梯度会失控地旋转，这被称为消失梯度或
爆炸梯度取决于梯度绝对值的方向
梯度绝对值非常大或非常小(小于1)都会使优化过程不稳定(Hochreiter et al.， 2001)。
在普通的神经网络中，有一些解决这些梯度问题的方法，如使用校正的线性单元(ReLUs)、梯度裁剪和仔细的初始化。但是没有一种解决方案像门控技术那样可靠。
门控作为一个解决方案香草RNN的挑战

为了直观地理解门控，假设你添加了两个量，a和b，但是你想要控制b进入和的多少。
λ是一个值在0和1之间。如果λ= 0,没有贡献从b如果λ= 1,b完全贡献。这样看,你可以解释λ充当一个“开关”或“门”在控制
进入和的b的数量，这是门控机制背后的直觉。现在让我们再来看看Elman RNN，看看如何将门控制合并到vanilla RNN中，使其具有条件
更新。如果之前的隐藏状态为h，当前的输入为x，那么Elman RNN中的循环更新会是这样的:
t−1 					t




其中F为RNN的递归计算。很明显，这是一个无条件的和，它有Vanilla RNNs(或Elman RNNs)问题中描述的弊端
常数,前面的示例的λ是以前隐藏的状态向量的函数h和当前输入x,而且还产生所需的控制行为;也就是说,一个值在0和1之间。有了这个门控函数，我们的RNN更新方程如下:
t−1


t




现在就清楚函数λ控制当前输入的多少可以更新状态h。进一步,函数λ是依赖于上下文­。这是所有大门背后的基本的直觉
网络。λ的函数通常是一个s形的函数,我们知道从hapter 3产生一个值在0和1之间。
t−1




在长短期记忆网络的情况下(LSTM In Hochreiter and Schmidhuber, 1997)，这种基本直觉被仔细地扩展，不仅包括条件更新，还包括有意更新
遗忘的价值观在前面的隐藏状态h。这个“忘记”乘以发生之前的隐藏状态值h与另一个函数,μ,还生产值在0和1之间
取决于当前输入:
t−1

t−1




您可能已经猜到,μ是另一个控制功能。在实际的LSTM描述,这就变成了

c tC " R
，因为门控函数是参数化的，这导致了一些复杂的
未启动的)操作序列。有了本节中的直觉知识，如果您想深入了解LSTM的更新机制，现在就可以开始了。我们推荐他的经典之作
hristopher Olah。在本书中，我们将不讨论这些内容，因为对于LSTMs在NLP应用程序中的应用和使用来说，细节并不重要。
LSTM只是RNN的众多门控变体之一
封闭式循环单元越来越受欢迎(GRU andchung et al.， 2015)。幸运的是，在PyTorch中，您可以简单地将nn.RNN或nn.RNNCell替换为nn.LSTM或nn.LSTMCell替换为no
其他代码更改以切换到LSTM(对GRU做了必要的修改)!
门控机制是对Vanilla神经网络(或Elman RNNs)问题中列举的问题的有效解决方案，它不仅控制了更新，而且保持了梯度问题
被检查，使训练相对容易。不再赘述，我们将使用两个示例展示这些门控体系结构的实际应用。
示例:用于生成姓氏的字符RNN

在本例中，我们将完成一个简单的序列预测任务:使用RNN生成姓氏。在实践中，这意味着对于每个时间步骤，RNN都在计算姓氏中可能的字符集上的概率分布。我们可以使用这些概率分布来优化网络以提高其预测(假设我们知道应该预测哪些字符)，或者生成全新的姓氏。
1




虽然这个任务的数据集在前面的示例中已经使用过，看起来很熟悉，但是还是有一些
序列预测中每个数据样本构造方式的一些差异。在描述了数据集和任务之后，我们概述了支持序列预测的数据结构
通过系统化的簿记。
然后我们介绍了两个生成姓氏的模型:无条件的
和条件的SurnameGenerationModel
非条件模型可以预测姓氏序列，而不需要知道姓氏的国籍。与此相反，条件模型利用特定的国籍嵌入作为RNN的初始隐藏状态，使得模型对序列的预测存在偏差。
SurnameDataset类

首先在例子中介绍:姓氏分类与一个MLP”，姓氏数据集是一个
收集姓氏和他们的原籍国。到目前为止，该数据集一直用于分类任务——给定一个新的姓氏，正确地将姓氏从哪个国家分类
起源。然而，在本例中，我们将展示如何使用数据集来训练一个可以为字符序列分配概率并生成新序列的模型。
SurnameDataset类与前几章基本相同:我们使用熊猫
构造一个矢量化器，该矢量化器封装模型和手头任务所需的令牌到整数的映射
__getitem__()方法被修改为输出预测目标的整数序列，如xample 7-1所示
计算作为输入的整数序列(from_vector)和作为输出的整数序列(to_vector)
在下一小节中描述。
例7-1 .序列预测任务的SurnameDataset.__getitem__()方法
类SurnameDataset(集):@classmethod
surname_csv def load_dataset_and_make_vectorizer (cls):
”““加载数据集,使一个新的vectorizer从零开始
参数:
surname_csv (str):数据集返回的位置:
的一个实例SurnameDataset”“”
        
surname_df = pd.read_csv (surname_csv)
返回cls(surname_df SurnameVectorizer.from_dataframe(surname_df)def __getitem__(自我,指数):
”““PyTorch数据集的主要入口点的方法
参数:
指数(int):索引数据点的回报:
字典的数据点:(x_data、y_target class_index”“”
行= self._target_df。iloc(指数)
from_vector, to_vector = \
self._vectorizer.vectorize(row.surname self._max_seq_length)
nationality_index = \
self._vectorizer.nationality_vocab.lookup_token(row.nationality)返回{ x_data:from_vector,
“y_target”:to_vector,
“class_index”:nationality_index }
向量化数据结构

与前面的示例一样，有三种主要的数据结构可以将每个姓氏的字符序列转换为其向量化的形式:序列词汇映射
令牌到整数，SurnameVectorizer协调整数映射
DataLoader将SurnameVectorizer的结果分组为小批
DataLoader实现及其使用在本例中保持不变，我们将跳过其实现细节
SURNAMEVECTORIZER和END-OF-SEQUENCE

对于序列预测任务，训练例程被编写为期望在每个时间步中有两个整数序列，分别代表令牌观测值和令牌目标

E c
想要预测我们正在训练的序列，例如本例中的姓氏。这意味着我们只有一个令牌序列可以使用，并通过打乱这个令牌序列来构造观察值和目标。
要将其转换为序列预测问题，需要使用
SequenceVocabulary。然后，序列开始令牌索引begin_seq_index被放在序列开始和序列结束令牌索引的前面，
将end_seq_index追加到序列的末尾，此时每个数据点为a
索引序列，具有相同的第一个和最后一个索引。要创建训练例程所需的输入和输出序列，只需使用索引序列的两个切片:第一个切片
包含除最后一个令牌索引外的所有令牌索引，第二个切片包含除第一个令牌索引外的所有令牌索引。
为了明确起见，我们在xample 7-2中显示了SurnameVectorizer.vectorize()的代码。第一步是将姓氏(字符串)映射到索引(表示这些字符的整数列表)。然后，用序列的开始和结束索引包装索引:具体来说，begin_seq_index加在索引前面，end_seq_index加在索引后面。
接下来，我们测试vector_length，它通常在运行时提供(尽管代码的编写允许向量的任何长度)。在训练过程中，提供vector_length是很重要的，因为小批是由堆叠的向量表示构造的
它们有不同的长度，它们不能堆放在一个矩阵中。在测试vector_length之后，创建两个向量:from_vector和to_vector
不包括最后一个索引放在from_vector中，不包括第一个索引的索引切片放在to_vector中。每个向量的其余位置都用mask_index填充，重要的是序列要向右填充(或填充)，因为
空位置将改变输出向量，我们希望这些变化发生在序列被看到之后。
示例7-2 .序列预测任务中SurnameVectorizer.vectorize()的代码
小姑娘SurnameVectorizer(对象):
用于协调词汇表并使其使用def vectorize(self，姓氏，vector_length= -1)的矢量化器:
”““Vectorize姓成一个向量的观察和目标
参数:
姓(str):姓矢量化
vector_length(int):一个论点迫使指数vect回报率的长度:
一个元组:(from_vector, to_vector)
from_vector(numpy.ndarray):观察向量
to_vector (numpy.ndarray):目标预测向量”“”
指数=[self.char_vocab.begin_seq_index]
index .extend(self.char_vocab.lookup_token(token) for token in姓).append(self.char_vocab.end_seq_index)
如果vector_length < 0:
vector_length = len(index) - 1
from_vector = np.zeros (vector_length dtype = np.int64) from_indices =指数[:­1]
from_vector [: len (from_indices)] = from_indices
from_vector len (from_indices): = self.char_vocab。mask_index to_vector = np.empty(vector_length dtype = np.int64)
to_indices =指数(1:)
to_vector [: len (to_indices)] = to_indices
to_vector len (to_indices): = self.char_vocab。mask_index
返回from_vector to_vector @classmethod
surname_df def from_dataframe (cls):
”“”从数据集dataframe vectorizer实例化
参数:
surname_df (pandas.DataFrame):姓氏数据集的回报:
的一个实例SurnameVectorizer”“”
char_vocab = SequenceVocabulary () nationality_vocab =词汇表()
指数,行surname_df.iterrows():为char row.surname:
char_vocab.add_token(字符)
nationality_vocab.add_token (row.nationality)返回cls (char_vocab nationality_vocab)
从ElmanRNN到GRU

在实践中，从普通的RNN切换到门控变量是非常容易的
虽然我们使用GRU来代替普通的RNN，但是使用LSTM也很简单。为了使用GRU，我们使用与ElmanRNN相同的参数实例化了torch.n .GRU模块
从hapter 6。
模型1:无条件的姓氏生成模型

第一个模型是无条件的:在生成a之前不观察国籍
实际上，无条件意味着GRU的计算不会偏向于任何一个
国籍。在下一个例子(xample 7-4)中，通过初始隐藏向量引入计算偏差，在这个例子中，我们使用一个全为0的向量，这样初始隐藏状态向量就不会出现偏差
对计算有所贡献
通常，SurnameGenerationModel (xample 7-3)嵌入字符索引并进行计算
它们的顺序状态使用GRU，并使用线性层计算令牌预测的概率
嵌入层、GRU层和线性层。与hapter 6的序列模型类似，我们在模型中输入一个整数矩阵
char_embed，将整数转换为一个三维张量(一个向量序列)
每一批项目)。这个张量传递给GRU, GRU为每个序列中的每个位置计算一个状态向量。

C
例7-3 .无条件姓氏生成模型
类SurnameGenerationModel(nn.Module):
def __init__(自我、char_embedding_size char_vocab_size、rnn_hidden_size batch_first = True,padding_idx = 0,dropout_p = 0.5):
”“”
参数:
char_embedding_size(int):字符的大小嵌入char_vocab_size(int):嵌入的字符数
rnn_hidden_size(int):RNN的隐藏状态的大小
batch_first(bool):通知张量的输入是否有批处理或序列在第0个维度
padding_idx(int):张量的指数填充;看到torch.nn.Embedding
dropout_p(浮动):归零法激活使用辍学的概率方法
”“”
超级(SurnameGenerationModel自我). __init__()
self.char_emb = nn.Embedding(num_embeddings = char_vocab_size,
embedding_dim = char_embedding_size padding_idx = padding_idx)
self.rnn = nn。格勒乌(input_size = char_embedding_size hidden_size = rnn_hidden_size,
batch_first = batch_first)
self.fc = nn。线性(in_features = rnn_hidden_size out_features = char_vocab_size)self._dropout_p = dropout_p
def向前(自我、x_in apply_softmax = False):““”模型的传球前进
        
参数:
张量x_in (torch.Tensor):一个输入数据
input_dim x_in.shape应该(批处理)
apply_softmax(bool):国旗培训期间将softmax激活应该是假的
返回:
结果tensor. tensor.shape应该(批处理,output_dim)。”“”
x_embedded = self.char_emb(x_in)y_out _ = self.rnn(x_embedded)
batch_size、seq_size feat_size = y_out.shape
y_out = y_out.contiguous().view(batch_size * seq_size feat_size)y_out = self.fc(F.dropout(y_out,p = self._dropout_p))
如果apply_softmax:
y_out = F.softmax(y_out昏暗= 1)
new_feat_size = y_out.shape­[1]
y_out = y_out.view(batch_size seq_size new_feat_size)
返回y_out
半抗原6序列分类任务与本章序列预测任务的主要区别在于如何处理RNN计算的状态向量。在第六章，我们
每个批处理索引检索一个向量，并使用这些向量执行预测
例如，我们将三维张量重塑为二维张量(一个矩阵)，这样行维就代表了每个样本(批处理和序列索引)
线性层，我们计算每个样本的预测向量。我们通过将矩阵重新构造成一个三维张量来完成计算
通过整形操作，每个批和序列索引都保持在相同的位置。我们需要整形的原因是因为线性层需要一个矩阵作为输入。
模型2:条件姓氏生成模型

第二种模型考虑了要生成的姓氏的国籍
在这个例子中，我们通过嵌入每个RNN的初始隐藏状态参数化
国籍作为一个矢量的大小隐含着国家。这意味着当模型调整参数时，它也会调整嵌入矩阵中的值，从而使预测偏向于对特定的国籍和姓氏的规律性更敏感。例如，爱尔兰国籍向量偏向于起始序列“Mc”和“O’”。
xample 7-4显示了条件模型中的差异
引入国籍索引映射到与RNN隐藏层大小相同的向量。然后，在正向函数中嵌入国籍指标，作为RNN的初始隐含层简单传入，虽然这是对第一个模型的一个非常简单的修改，但在
让RNN根据生成的姓氏的国籍更改其行为。例7-4 .条件姓氏生成模型
类SurnameGenerationModel(nn.Module):
(self, char_embed _size, char_vocab_size, num_国籍，
rnn_hidden_size batch_first = True,padding_idx = 0,dropout_p = 0)。#……
self.nation_embedding = nn.Embedding(embedding_dim = rnn_hidden_size,
num_embeddings = num_nationalities)def向前(自我、x_in nationality_index apply_softmax = False):
#……
x_embedded = self.char_embedding(x_in)
# hidden_size:(num_layers * num_directions, batch_size, rnn_hidden_siz) nationality_embedded = self.nation_emb(nationality_index).unsqueeze(0)
y_out _ = self.rnn(x_embedded nationality_embedded)#……
训练程序和结果

在本例中，我们介绍了用于生成姓氏的字符序列预测任务。虽然在许多方面，实现细节和训练例程与hapter 6中的序列分类示例类似，但有一些主要的区别。在本节中，我们将重点讨论差异、使用的超参数和结果。
与前面的示例相比，计算此示例中的损失需要两个更改

E
因为我们在序列的每一步都在做预测
维张量到二维张量(矩阵)满足计算约束。
其次，我们将允许可变长度序列的屏蔽索引与损失函数进行协调，使损失在计算中不使用屏蔽位置。
4




我们处理这两个问题-三维张量和变长序列-利用
代码片段如xample 7-5所示。首先，预测和目标的大小被规范化
损失函数期望(预测是二维的，目标是一维的)。现在，每一行代表一个样本:一个序列中的一个时间步长，然后使用交叉熵损失
ignore_index设置为mask_index。这将导致loss函数忽略与ignore_index匹配的目标中的任何位置。
示例7-5 .处理三维张量和全序列损失计算
正常化张量大小(y_pred, y_true):“
    
参数:
y_pred(torch.Tensor):模型的输出
如果3­维张量,重塑了一个矩阵y_true(torch.Tensor):目标预测
如果一个矩阵,重塑了向量”“”
如果len(y_pred.size()) == 3:
y_pred = y_pred.contiguous().view(­1,y_pred.size(2)如果len(y_true.size())= = 2:
y_true = y_true.contiguous().view(­1)返回y_pred y_true
def sequence_loss(y_pred, y_true, mask_index):
y_pred y_true = normalize_sizes(y_pred y_true)
返回F.cross_entropy(y_pred, y_true, ignore_index=mask_index)
使用这种修改后的损失计算，我们构建了一个训练例程，它与本书中其他所有示例中的训练例程类似，它首先在训练数据集上进行迭代，每次处理一个小批。对于每个小批处理，模型的输出都是从输入计算的，因为我们正在执行
预测每一步，模型的输出是一个三维张量。使用前面描述的sequence_loss()和优化器，可以计算模型预测的错误信号，并用于更新模型参数。
大多数模型超参数是由字符词汇表的大小决定的。这个大小是可以观察到的作为模型输入的离散标记的数量和其中的类的数量
每个时间步的输出分类。剩下的模型超参数是字符嵌入的大小和内部RNN隐藏状态的大小。xample 7-6展示了这些
超参数和训练选项。
例7-6 .姓氏生成的超参数
args =名称空间(
数据和路径信息
数据/姓氏/ surnames_with_splits surname_csv = "。csv”,vectorizer_file = " vectorizer.json”,
model_state_file = " model.pth ",
save_dir = " model_storage / ch7 model1_unconditioned_surname_generation”,
或者:save_dir="model_storage/ch7/model2_conditioned_surname_generation"， # Model超参数
char_embedding_size = 32,rnn_hidden_size = 32,
训练超参数seed=1337，
batch_size learning_rate = 0.001,= 128,
num_epochs = 100,
early_stopping_criteria = 5,
为空格省略了运行时选项)




即使预测的每个字符的准确性是模型性能的一个度量，它确实是
在本例中，最好通过检查模型将生成哪些姓氏来进行定性评估。
方法计算每个时间步长的预测，并将这些预测用作下一个时间步的输入。我们将在xample 7-7中显示代码。模型每一步的输出是一个预测向量，利用softmax函数将预测向量转化为概率分布。利用概率分布，利用火炬多项式()抽样函数进行选择
指数的速率与指数的概率成正比。抽样是每次产生不同输出的随机过程。
示例7-7 .从无条件生成模型中取样
def sample_from_model(模型、vectorizer num_samples = 1, sample_size = 20,温度= 1.0):
”““样本序列的指数模型
参数:
模型(SurnameGenerationModel):训练的模型
vectorizer (SurnameVectorizer):对应的vectorizer num_samples (int):样本数量
sample_size (int):样品的最大长度
温度(浮动):强调了或者变平时分布温度0.0 < < 1.0将憔悴的
温度> 1.0将使它更加统一的返回:
指数(torch.Tensor):矩阵的形状指数= (num_samples sample_size)
”“”
(vectorizer.char_vocab begin_seq_index =。begin_seq_index _的范围(num_samples))
begin_seq_index = torch.tensor (begin_seq_index,
dtype = torch.int64) .unsqueeze(暗= 1)指数= [begin_seq_index]
h_t =没有
time_step的范围(sample_size): x_t =指数(time_step)
x_emb_t = model.char_emb(x_t)
rnn_out_t, h_t = model.rnn(x_emb_t, h_t)
prediction_vector = model.fc (rnn_out_t.squeeze(暗= 1)
index .append(火炬多项式(probability ty_vector, num_samples=1))
index =火炬。堆栈(index).挤压().置换(1,0)返回索引
我们需要将采样的索引从sample_from_model()函数转换为一个字符串，以供人类可读输出。如xample 7-8所示，为此，我们使用
用于向量化姓氏的序列词汇。在创建字符串时，我们只使用序列结束索引之前的索引
姓氏何时结束。
示例7-8 .将采样索引映射到姓氏字符串
def decode_samples (sampled_indices vectorizer):
”“指标转换成字符串形式的姓
参数:
sampled_indices(火炬。张量):“sample_from_model”矢量化器(SurnameVectorizer)的索引:对应的矢量化器
”“”
decoded_surnames = []
词汇= vectorizer。char_vocab
sample_index的范围(sampled_indices.shape[0]):姓= " "
time_step的范围(sampled_indices.shape [1]):
sample_item = sampled_indices [sample_index time_step] .item如果sample_item = = vocab.begin_seq_index ():
继续
elif sample_item == vocab。end_seq_index:打破
其他:
姓+ = vocab.lookup_index (sample_item) decoded_surnames.append(姓氏)
返回decoded_surnames
使用这些函数，您可以检查模型的输出，如xample 7-9所示，以了解其中的含义
模型是否正在学习生成合理的姓氏。通过检查输出，我们可以了解到什么
名字没有明显的民族特征，一种可能性是学习a
姓氏的一般模型混淆了不同民族之间的性格分布。有条件的SurnameGenerationModel就是用来处理这种情况的。
示例7-9 .从非条件模型中取样

输入[0]samples = sample_from_model(unconditioned_model, vectorizer，
vectorizer num_samples = 10)decode_samples(样本)
 
对于有条件的SurnameGenerationModel，我们修改sample_from_model()
函数的作用是:接受国籍索引列表，而不是指定数量的样本。在xample 7 - 0中，修改后的函数使用具有嵌入国籍的国籍索引来构造
GRU的初始隐藏状态。之后，采样过程与无条件模型完全相同。
示例7-10 .从序列模型中取样
def sample_from_model(模型、vectorizer民族sample_size = 20,温度= 1.0):
”““样本序列的指数模型
参数:
模型(SurnameGenerationModel):训练的模型
矢量化器(SurnameVectorizer):对应的矢量化器
国籍(列表):表示国籍的整数列表sample_size (int):样本的最大长度
温度(浮动):强调了或者变平时分布温度0.0 < < 1.0将憔悴的
温度> 1.0将使它更加统一的返回:
指数(torch.Tensor):矩阵的形状指数= (num_samples sample_size)
”“”
num_samples = len(民族)
(vectorizer.char_vocab begin_seq_index =。begin_seq_index _的范围(num_samples))
begin_seq_index = torch.tensor (begin_seq_index,
dtype = torch.int64) .unsqueeze(暗= 1)指数= [begin_seq_index]
nationality_indices = torch.tensor(民族、
dtype = torch.int64) .unsqueeze(暗= 0)h_t = model.nation_emb (nationality_indices)
    
time_step的范围(sample_size): x_t =指数(time_step)
x_emb_t = model.char_emb(x_t)
rnn_out_t, h_t = model.rnn(x_emb_t, h_t)
prediction_vector = model.fc (rnn_out_t.squeeze(暗= 1)
index .append(火炬多项式(probability ty_vector, num_samples=1))
index =火炬。堆栈(index).挤压().置换(1,0)返回索引
条件向量抽样的有用性意味着我们对
生成的输出。在xample 7-11中，我们遍历国籍索引并对每个索引进行采样。为了节省空间，我们只显示少数输出。从这些输出中，我们可以看到
这个模型确实在研究姓氏的一些拼写模式。
示例7-11 .从条件SurnameGenerationModel中取样(并非显示所有输出)
输入[0]for index in range(len(vectorizer.nationality_vocab)):
国籍= vectorizer.nationality_vocab.lookup_index(index) print(" sample for {}: ".format(国籍))
sampled_indices = sample_from_model(模型= conditioned_model vectorizer = vectorizer,
民族=(指数)* 3、温度= 0.7)
在decode_samples sampled_surname (sampled_indices vectorizer):
print(" - " + sampled_姓氏)
输出[0]阿拉伯语采样:
- Khatso - Salbwa - Gadi
中文样本:-撒谎
­Puh
­雅司病
德国取样:-更长
- Schanger - Schumper
为爱尔兰人取样:- Mcochin
­Corran
­O 'Baintin
俄语样本:- Mahghatsunkov
­Juhin
­Karkovin
越南语样本:- Lo
——谭头

训练序列模型的提示和技巧

序列模型很难训练，在这个过程中会出现很多问题
总结一些我们在工作中发现有用的技巧和技巧，以及其他人在文献中报道的技巧和技巧。
如果可能，使用门控变量
门控体系结构通过解决许多非整数变量的数值稳定性问题简化了训练。
如果可能的话，选择GRUs而不是LSTMs
GRUs提供了几乎与LSTMs相当的性能，并且使用了更少的参数和
计算资源。幸运的是，从PyTorch的角度来看，使用GRU而不是LSTM只需要使用不同的模块类。
使用Adam作为优化器
在第6、7和8章中，我们只使用Adam作为优化器，这是有原因的:它是可靠的，而且通常比其他方法收敛得更快
模型。如果由于某种原因你的模型没有和Adam收敛，那么切换到随机梯度下降可能会有所帮助。
梯度剪裁
如果你在应用这些章节中所学的概念时注意到数值上的错误，在训练过程中，用你的代码绘制梯度的值。在PyTorch中有一个有用的实用程序clip_grad_norm()
如xample 7-12所示，这是为您准备的。一般来说，你应该养成剪切渐变的习惯。


示例7-12 .在PyTorch中应用渐变裁剪
定义你的序列模型模型= ..
定义loss function loss_function = ..
_ in…的训练循环:
…
model.zero_grad ()
输出，隐藏=模型(数据，隐藏)
loss = loss_function(output, targets) loss. reverse ()
torch.nn.utils.clip_grad_norm(model.parameters(),0.25)……
早期停止
对于序列模型，很容易过度拟合。我们建议您尽早停止培训过程，当在开发集上测量的评估错误开始出现时。
在第8章中，我们继续讨论序列模型，探索如何使用序列到序列模型预测和生成长度不同于输入的序列，并考虑
其他变体。
参考文献

1.霍克莱特、塞普和约根施米德胡伯(1997)。神经计算。
2.Hochreiter, Sepp et al.(2001).《循环网中的梯度流:学习的困难》
《动态循环神经网络的现场指南》。IEEE出版社。


3.Pascanu, Razvan, Tomas Mikolov, yoshu Bengio.(2013)。关于训练递归神经网络的困难>，《第三十届国际机器会议论文集》
学习。


4.Chung, Junyoung等(2015). <门控反馈递归神经网络>。“第32届机器学习国际会议论文集”。
1本书itHub中的/chapter /chapter_7/7_3_surname_generation中提供了代码
促红细胞生成素。


我们建议您参考词汇表、矢量化器和数据阅读器“以获得更深入的了解”
顺序evocabulary和词汇表、矢量化器和数据阅读器”，用于介绍词汇表和矢量化器数据结构。
如果初始的隐藏向量都是0，矩阵乘以它只会得到0。
三维张量在第一个维度为批处理，在第二个维度为序列
第三个预测向量。


